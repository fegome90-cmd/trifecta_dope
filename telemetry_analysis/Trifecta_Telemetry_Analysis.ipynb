{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# \ud83d\udd2c Trifecta Telemetry Analysis\n",
    "\n",
    "This notebook provides comprehensive analysis of Trifecta CLI telemetry data including:\n",
    "- Performance metrics (P50/P95/P99 latencies)\n",
    "- Command usage patterns  \n",
    "- Search effectiveness\n",
    "- Cold vs Warm run comparisons\n",
    "- Token usage analysis\n",
    "\n",
    "---\n",
    "## 1. Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cell-2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd27 Environment Check\n",
      "==================================================\n",
      "Python: 3.14.2\n",
      "Working directory: /Users/felipe_gonzalez/Developer/agent_h/trifecta_dope/telemetry_analysis\n",
      "\u2705 Auto-detected repo root: /Users/felipe_gonzalez/Developer/agent_h/trifecta_dope\n",
      "\n",
      "Analyzing last 7 days\n"
     ]
    }
   ],
   "source": [
    "print(\"\ud83d\udd27 Environment Check\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Python: {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\")\n",
    "print(f\"Working directory: {Path.cwd()}\")\n",
    "\n",
    "# Auto-detect repository root by looking for telemetry directory\n",
    "possible_paths = [\n",
    "    Path(\"..\"),                          # telemetry_analysis/ -> repo root\n",
    "    Path.cwd().parent,                   # Same as above, explicit\n",
    "    Path(\".\"),                           # Current directory (for testing)\n",
    "    Path(\"../trifecta_dope\"),             # debug_terminal/ -> repo root\n",
    "]\n",
    "\n",
    "REPO_ROOT = None\n",
    "for path in possible_paths:\n",
    "    if (path / \"_ctx\" / \"telemetry\" / \"events.jsonl\").exists():\n",
    "        REPO_ROOT = path.resolve()\n",
    "        break\n",
    "\n",
    "if REPO_ROOT is None:\n",
    "    raise RuntimeError(\n",
    "        \"Cannot proceed without valid telemetry path. \"\n",
    "        \"Expected _ctx/telemetry/events.jsonl not found in any searched location. \"\n",
    "        \"Please run this notebook from the trifecta_dope directory.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"\u2705 Auto-detected repo root: {REPO_ROOT}\")\n",
    "\n",
    "# Analysis window: 7 days provides recent data while ensuring sufficient volume\n",
    "# Set to 0 for all-time analysis (may be slow for large datasets)\n",
    "DAYS = 7\n",
    "\n",
    "# Operations for cold/warm performance comparison\n",
    "# Maps display name -> (section, operation, metric_key) for navigating telemetry JSON\n",
    "# Used to extract timing data from last_run_cold.json and last_run_warm.json\n",
    "OPERATIONS = {\n",
    "    'Context Build': ('ctx', 'build', 'total_time_ms'),  # Full context rebuild\n",
    "    'AST Parse': ('ast', 'parse', 'total_time_ms'),       # AST parsing operation\n",
    "    'Context Sync': ('ctx', 'sync', 'total_time_ms'),     # Context sync to disk\n",
    "}\n",
    "\n",
    "print(f\"\\nAnalyzing last {DAYS} days\" if DAYS > 0 else \"Analyzing all data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## Load Telemetry Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cell-4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u274c Unexpected error loading telemetry!\n",
      "   Error type: TypeError\n",
      "   Message: Invalid comparison between dtype=datetime64[ns, UTC-03:00] and datetime\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid comparison between dtype=datetime64[ns, UTC-03:00] and datetime",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimelike.py:559\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    558\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m559\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_compatible_with\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    560\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m     \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimes.py:542\u001b[39m, in \u001b[36mDatetimeArray._check_compatible_with\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    541\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m542\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_assert_tzawareness_compat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimes.py:788\u001b[39m, in \u001b[36mDatetimeArray._assert_tzawareness_compat\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m other_tz \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    789\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot compare tz-naive and tz-aware datetime-like objects\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    790\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: Cannot compare tz-naive and tz-aware datetime-like objects",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mInvalidComparison\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimelike.py:1006\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1005\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1006\u001b[39m     other = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_comparison_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimelike.py:562\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._validate_comparison_value\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    560\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, IncompatibleFrequency) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    561\u001b[39m         \u001b[38;5;66;03m# e.g. tzawareness mismatch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m562\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidComparison(other) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(other):\n",
      "\u001b[31mInvalidComparison\u001b[39m: 2025-12-28 11:18:12.038945",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 114\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;66;03m# Load the data with error handling\u001b[39;00m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m     events_df = \u001b[43mload_telemetry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mREPO_ROOT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDAYS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\u2705 Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(events_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m events\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m events_df[\u001b[33m'\u001b[39m\u001b[33mtotal_tokens\u001b[39m\u001b[33m'\u001b[39m].sum() == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 107\u001b[39m, in \u001b[36mload_telemetry\u001b[39m\u001b[34m(repo_root, days)\u001b[39m\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m days > \u001b[32m0\u001b[39m:\n\u001b[32m    106\u001b[39m     cutoff = datetime.now() - timedelta(days=days)\n\u001b[32m--> \u001b[39m\u001b[32m107\u001b[39m     df = df[\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtimestamp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m]\n\u001b[32m    108\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFiltered to last \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdays\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m days (since \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcutoff.date()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/series.py:6138\u001b[39m, in \u001b[36mSeries._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   6135\u001b[39m lvalues = \u001b[38;5;28mself\u001b[39m._values\n\u001b[32m   6136\u001b[39m rvalues = extract_array(other, extract_numpy=\u001b[38;5;28;01mTrue\u001b[39;00m, extract_range=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m6138\u001b[39m res_values = \u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomparison_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._construct_result(res_values, name=res_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/ops/array_ops.py:330\u001b[39m, in \u001b[36mcomparison_op\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    322\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLengths must match to compare\u001b[39m\u001b[33m\"\u001b[39m, lvalues.shape, rvalues.shape\n\u001b[32m    323\u001b[39m         )\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_extension_dispatch(lvalues, rvalues) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m    326\u001b[39m     (\u001b[38;5;28misinstance\u001b[39m(rvalues, (Timedelta, BaseOffset, Timestamp)) \u001b[38;5;129;01mor\u001b[39;00m right \u001b[38;5;129;01mis\u001b[39;00m NaT)\n\u001b[32m    327\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m lvalues.dtype != \u001b[38;5;28mobject\u001b[39m\n\u001b[32m    328\u001b[39m ):\n\u001b[32m    329\u001b[39m     \u001b[38;5;66;03m# Call the method on lvalues\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m     res_values = \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_scalar(rvalues) \u001b[38;5;129;01mand\u001b[39;00m isna(rvalues):  \u001b[38;5;66;03m# TODO: but not pd.NA?\u001b[39;00m\n\u001b[32m    333\u001b[39m     \u001b[38;5;66;03m# numpy does not like comparisons vs None\u001b[39;00m\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op \u001b[38;5;129;01mis\u001b[39;00m operator.ne:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/ops/common.py:76\u001b[39m, in \u001b[36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[32m     74\u001b[39m other = item_from_zerodim(other)\n\u001b[32m---> \u001b[39m\u001b[32m76\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arraylike.py:60\u001b[39m, in \u001b[36mOpsMixin.__ge__\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m__ge__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__ge__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cmp_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mge\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/arrays/datetimelike.py:1008\u001b[39m, in \u001b[36mDatetimeLikeArrayMixin._cmp_method\u001b[39m\u001b[34m(self, other, op)\u001b[39m\n\u001b[32m   1006\u001b[39m     other = \u001b[38;5;28mself\u001b[39m._validate_comparison_value(other)\n\u001b[32m   1007\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidComparison:\n\u001b[32m-> \u001b[39m\u001b[32m1008\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minvalid_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1010\u001b[39m dtype = \u001b[38;5;28mgetattr\u001b[39m(other, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1011\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(dtype):\n\u001b[32m   1012\u001b[39m     \u001b[38;5;66;03m# We have to use comp_method_OBJECT_ARRAY instead of numpy\u001b[39;00m\n\u001b[32m   1013\u001b[39m     \u001b[38;5;66;03m#  comparison otherwise it would raise when comparing to None\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Developer/agent_h/trifecta_dope/.venv/lib/python3.14/site-packages/pandas/core/ops/invalid.py:40\u001b[39m, in \u001b[36minvalid_comparison\u001b[39m\u001b[34m(left, right, op)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     39\u001b[39m     typ = \u001b[38;5;28mtype\u001b[39m(right).\u001b[34m__name__\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid comparison between dtype=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mleft.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtyp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "\u001b[31mTypeError\u001b[39m: Invalid comparison between dtype=datetime64[ns, UTC-03:00] and datetime"
     ]
    }
   ],
   "source": [
    "def load_telemetry(repo_root: Path, days: int = 7) -> pd.DataFrame:",
    "    \"\"\"Load and parse telemetry events from events.jsonl into a pandas DataFrame.",
    "    ",
    "    This function:",
    "    - Loads JSONL-format telemetry events",
    "    - Parses timestamps and filters by date range",
    "    - Extracts nested fields (status, hits, tokens, warnings)",
    "    - Handles missing columns and malformed data gracefully",
    "    ",
    "    Args:",
    "        repo_root: Repository root path containing _ctx/telemetry/ directory",
    "        days: Number of days to analyze (0 = all data, N = last N days)",
    "    ",
    "    Returns:",
    "        DataFrame with columns: timestamp, date, timing_ms, status, hits, ",
    "                                total_tokens, retrieved_tokens, warnings_count, cmd",
    "    ",
    "    Raises:",
    "        FileNotFoundError: If telemetry file doesn't exist at expected path",
    "        ValueError: If no events found or required columns are missing",
    "    \"\"\"",
    "    tel_dir = repo_root / \"_ctx\" / \"telemetry\"",
    "    events_path = tel_dir / \"events.jsonl\"",
    "    ",
    "    if not events_path.exists():",
    "        raise FileNotFoundError(f\"Telemetry file not found: {events_path}\")",
    "    ",
    "    # Load events with error logging",
    "    events = []",
    "    decode_errors = 0",
    "    with open(events_path, 'r') as f:",
    "        for line_num, line in enumerate(f, 1):",
    "            if line.strip():",
    "                try:",
    "                    events.append(json.loads(line))",
    "                except json.JSONDecodeError as e:",
    "                    decode_errors += 1",
    "                    if decode_errors <= 3:  # Show first 3 errors",
    "                        print(f\"\u26a0\ufe0f  JSON decode error at line {line_num}: {e}\")",
    "    ",
    "    if decode_errors > 0:",
    "        print(f\"\u26a0\ufe0f  Total JSON decode errors: {decode_errors} (skipped)\")",
    "    ",
    "    if not events:",
    "        raise ValueError(f\"No events found in {events_path}\")",
    "    ",
    "    # Convert to DataFrame",
    "    df = pd.DataFrame(events)",
    "    ",
    "    # Handle malformed timestamps by coercing to NaT, then drop invalid rows",
    "    # This prevents AttributeError when accessing .dt.date later",
    "    # Normalize all timestamps to UTC naive to avoid timezone comparison issues",
    "    df['timestamp'] = pd.to_datetime(df['ts'], errors='coerce', utc=True).dt.tz_localize(None)",
    "    before_count = len(df)",
    "    na_count = df['timestamp'].isna().sum()",
    "    ",
    "    if na_count > 0:",
    "        print(f\"\u26a0\ufe0f  Dropping {na_count} events with unparseable timestamps\")",
    "    ",
    "    df = df.dropna(subset=['timestamp'])",
    "    df['date'] = df['timestamp'].dt.date",
    "    ",
    "    # Validate required columns exist before accessing (telemetry schema may vary)",
    "    required_cols = ['timing_ms', 'result', 'warnings', 'cmd']",
    "    missing_cols = [col for col in required_cols if col not in df.columns]",
    "    if missing_cols:",
    "        print(f\"\u274c Schema validation failed!\")",
    "        print(f\"   Missing columns: {missing_cols}\")",
    "        print(f\"   Available columns: {list(df.columns)}\")",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")",
    "    ",
    "    # Extract nested fields",
    "    df['timing_ms'] = df['timing_ms'].fillna(0).astype(int)",
    "    df['status'] = df['result'].apply(lambda x: x.get('status', 'unknown') if isinstance(x, dict) else 'unknown')",
    "    df['hits'] = df['result'].apply(lambda x: x.get('hits', 0) if isinstance(x, dict) else 0)",
    "    ",
    "    # Token extraction from telemetry events",
    "    # Schema: tokens field may be missing (older events) or present (newer events)",
    "    # Expected structure when present: {'total_tokens': int, 'retrieved_tokens': int}",
    "    # Commands with tokens: ctx.search, ctx.build (varies by command type)",
    "    if 'tokens' in df.columns:",
    "        def safe_get_tokens(row):",
    "            \"\"\"Extract token counts from a telemetry event row.",
    "            ",
    "            Args:",
    "                row: DataFrame row (Series) with 'tokens' column containing dict or NaN",
    "            ",
    "            Returns:",
    "                Tuple of (total_tokens, retrieved_tokens), defaulting to (0, 0)",
    "            \"\"\"",
    "            tokens = row.get('tokens', {}) if isinstance(row, dict) else row",
    "            if isinstance(tokens, dict):",
    "                return tokens.get('total_tokens', 0), tokens.get('retrieved_tokens', 0)",
    "            return 0, 0",
    "        ",
    "        token_data = df.apply(safe_get_tokens, axis=1, result_type='expand')",
    "        df['total_tokens'] = token_data[0].astype(int)",
    "        df['retrieved_tokens'] = token_data[1].astype(int)",
    "    else:",
    "        df['total_tokens'] = 0",
    "        df['retrieved_tokens'] = 0",
    "    ",
    "    df['warnings_count'] = df['warnings'].apply(lambda x: len(x) if isinstance(x, list) else 0)",
    "    ",
    "    # Filter by date",
    "    # Note: cutoff uses local time, all timestamps are normalized to UTC",
    "    if days > 0:",
    "        cutoff = pd.Timestamp(datetime.now()).tz_localize(None)",
    "        df = df[df['timestamp'] >= cutoff]",
    "        print(f\"Filtered to last {days} days (since {cutoff.date()})\")",
    "    ",
    "    return df",
    "",
    "# Load the data with error handling",
    "try:",
    "    events_df = load_telemetry(REPO_ROOT, DAYS)",
    "    print(f\"\\n\u2705 Loaded {len(events_df)} events\")",
    "    ",
    "    if events_df['total_tokens'].sum() == 0:",
    "        print(\"\u26a0\ufe0f  No token data available in telemetry events\")",
    "except FileNotFoundError as e:",
    "    print(f\"\u274c Telemetry file not found!\")",
    "    print(f\"   Searched path: {REPO_ROOT / '_ctx' / 'telemetry'}\")",
    "    print(f\"   Working directory: {Path.cwd()}\")",
    "    print(f\"   Please ensure telemetry data exists\")",
    "    raise",
    "except ValueError as e:",
    "    print(f\"\u274c Data validation error!\")",
    "    print(f\"   Issue: {e}\")",
    "    print(f\"   Check telemetry format and version compatibility\")",
    "    raise",
    "except Exception as e:",
    "    print(f\"\u274c Unexpected error loading telemetry!\")",
    "    print(f\"   Error type: {type(e).__name__}\")",
    "    print(f\"   Message: {e}\")",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Data Overview\n",
    "\n",
    "This section provides basic statistics about the telemetry dataset:\n",
    "- Total event volume and date range\n",
    "- Command distribution  \n",
    "- Run identification (if available)\n",
    "\n",
    "**What to look for:**\n",
    "- Sufficient data volume (100+ events recommended for statistical significance)\n",
    "- Recent data (check date range matches expectations)\n",
    "- Expected command types (ctx.build, ctx.search, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if events_df.empty:\n",
    "    print(\"\u26a0\ufe0f  No data available after filtering!\")\n",
    "else:\n",
    "    print(\"\ud83d\udcca Data Overview\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Events:       {len(events_df):,}\")\n",
    "    print(f\"Date Range:         {events_df['date'].min()} to {events_df['date'].max()}\")\n",
    "    print(f\"Unique Commands:    {events_df['cmd'].nunique()}\")\n",
    "    \n",
    "    # run_id column may not exist in all telemetry versions\n",
    "    if 'run_id' in events_df.columns:\n",
    "        print(f\"Total Runs:         {events_df['run_id'].nunique()}\")\n",
    "    else:\n",
    "        print(f\"Total Runs:         N/A (no run_id column)\")\n",
    "    \n",
    "    print(f\"\\nCommand Types:\")\n",
    "    print(events_df['cmd'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Performance Analysis\n",
    "\n",
    "Analyzes latency distributions across commands to identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "timed = events_df[events_df['timing_ms'] > 0].copy()\n",
    "print(f\"Events with timing data: {len(timed)}\")\n",
    "\n",
    "if not timed.empty:\n",
    "    percentiles = timed.groupby('cmd')['timing_ms'].agg([\n",
    "        ('Count', 'count'),\n",
    "        ('P50', 'median'),\n",
    "        ('P95', lambda x: x.quantile(0.95)),\n",
    "        ('P99', lambda x: x.quantile(0.99)),\n",
    "        ('Mean', 'mean'),\n",
    "        ('Max', 'max')\n",
    "    ]).round(1)\n",
    "    \n",
    "    percentiles = percentiles.sort_values('Count', ascending=False)\n",
    "    percentiles\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No timing data available\")\n",
    "    # Initialize empty DataFrame to prevent NameError if cells run out of order\n",
    "    percentiles = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not timed.empty:\n",
    "    top_cmds = timed['cmd'].value_counts().head(10).index\n",
    "    filtered = timed[timed['cmd'].isin(top_cmds)]\n",
    "    \n",
    "    fig = px.box(\n",
    "        filtered, \n",
    "        x='cmd', \n",
    "        y='timing_ms', \n",
    "        title='Latency Distribution by Command (Top 10)',\n",
    "        labels={'cmd': 'Command', 'timing_ms': 'Latency (ms)'}\n",
    "    )\n",
    "    fig.update_layout(yaxis_type=\"log\", height=500)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No timing data available for chart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not timed.empty:",
    "",
    "    fig = px.histogram(",
    "",
    "        timed,",
    "",
    "        x='timing_ms',",
    "",
    "        title='Overall Latency Distribution',",
    "",
    "        labels={'timing_ms': 'Latency (ms)'},",
    "",
    "        nbins=50,",
    "",
    "        log_y=True",
    "",
    "    )",
    "",
    "    fig.update_layout(xaxis_type=\"log\")",
    "",
    "    fig.show()",
    "",
    "else:",
    "",
    "    print(\"\u26a0\ufe0f  No timing data available for chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. Command Usage Patterns\n",
    "\n",
    "Examines which commands are used most frequently to understand usage patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_counts = events_df['cmd'].value_counts()\n",
    "\n",
    "fig = px.pie(\n",
    "    values=cmd_counts.values, \n",
    "    names=cmd_counts.index, \n",
    "    title='Command Distribution',\n",
    "    hole=0.3\n",
    ")\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    x=cmd_counts.index,\n",
    "    y=cmd_counts.values,\n",
    "    title='Command Frequency',\n",
    "    labels={'x': 'Command', 'y': 'Count'},\n",
    "    text=cmd_counts.values\n",
    ")\n",
    "fig.update_traces(textposition='outside')\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## 5. Search Effectiveness\n",
    "\n",
    "Analyzes ctx.search hit rates to evaluate search quality.\n",
    "\n",
    "**Gauge thresholds:**\n",
    "- **< 50%**: Poor performance (red zone)\n",
    "- **50-80%**: Acceptable (yellow zone)  \n",
    "- **> 80%**: Target (green zone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "searches = events_df[events_df['cmd'] == 'ctx.search'].copy()\n",
    "\n",
    "# Initialize with default value - allows downstream cells to check safely\n",
    "# even if this cell is re-run out of order\n",
    "hit_rate = None\n",
    "\n",
    "if len(searches) > 0:\n",
    "    total_searches = len(searches)\n",
    "    with_hits = (searches['hits'] > 0).sum()\n",
    "    hit_rate = (with_hits / total_searches * 100) if total_searches > 0 else 0\n",
    "    \n",
    "    print(f\"\ud83d\udd0d Search Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total Searches:      {total_searches}\")\n",
    "    print(f\"With Hits:           {with_hits} ({hit_rate:.1f}%)\")\n",
    "    print(f\"Zero Hits:           {total_searches - with_hits} ({100-hit_rate:.1f}%)\")\n",
    "    \n",
    "    fig = go.Figure(go.Indicator(\n",
    "        mode=\"gauge+number\",\n",
    "        value=hit_rate,\n",
    "        title={'text': \"Search Hit Rate (%)\"},\n",
    "        gauge={\n",
    "            'axis': {'range': [0, 100]},\n",
    "            'bar': {'color': \"darkblue\"},\n",
    "            'steps': [\n",
    "                {'range': [0, 50], 'color': \"lightgray\"},   # Below 50%: poor performance\n",
    "                {'range': [50, 80], 'color': \"gray\"},       # 50-80%: acceptable\n",
    "            ],\n",
    "            'threshold': {\n",
    "                'line': {'color': \"red\", 'width': 4},\n",
    "                'thickness': 0.75,\n",
    "                'value': 80  # 80% is the target for good search effectiveness\n",
    "            }\n",
    "        }\n",
    "    ))\n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No search events found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'searches' in locals() and len(searches) > 0:\n",
    "    # Use named aggregation to avoid FutureWarning from deprecated .apply()\n",
    "    daily_hit_rate = searches.groupby('date').agg(\n",
    "        hit_rate=('hits', lambda x: (x > 0).sum() / len(x) * 100)\n",
    "    ).reset_index()\n",
    "    \n",
    "    fig = px.line(\n",
    "        daily_hit_rate,\n",
    "        x='date',\n",
    "        y='hit_rate',\n",
    "        title='Daily Search Hit Rate',\n",
    "        labels={'date': 'Date', 'hit_rate': 'Hit Rate (%)'},\n",
    "        markers=True\n",
    "    )\n",
    "    fig.update_layout(height=400)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No search events available for chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Cold vs Warm Comparison\n",
    "\n",
    "Analyzes cache effectiveness by comparing cold (no cache) vs warm (cached) run performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "tel_dir = REPO_ROOT / \"_ctx\" / \"telemetry\"\n",
    "\n",
    "cold_run = None\n",
    "warm_run = None\n",
    "\n",
    "cold_path = tel_dir / \"last_run_cold.json\"\n",
    "warm_path = tel_dir / \"last_run_warm.json\"\n",
    "\n",
    "if cold_path.exists():\n",
    "    with open(cold_path) as f:\n",
    "        cold_run = json.load(f)\n",
    "    print(f\"\u2705 Loaded cold run data: {cold_path}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cold run data not found\")\n",
    "\n",
    "if warm_path.exists():\n",
    "    with open(warm_path) as f:\n",
    "        warm_run = json.load(f)\n",
    "    print(f\"\u2705 Loaded warm run data: {warm_path}\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Warm run data not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cold_run and warm_run:\n",
    "    comparison_data = []\n",
    "    \n",
    "    for display_name, (section, op, metric) in OPERATIONS.items():\n",
    "        cold_time = cold_run.get(section, {}).get(op, {}).get(metric, 0)\n",
    "        warm_time = warm_run.get(section, {}).get(op, {}).get(metric, 0)\n",
    "        speedup = cold_time / warm_time if warm_time > 0 else 1.0\n",
    "        \n",
    "        comparison_data.append({\n",
    "            'Operation': display_name,\n",
    "            'Cold (ms)': cold_time,\n",
    "            'Warm (ms)': warm_time,\n",
    "            'Speedup': f\"{speedup:.2f}x\" if speedup != 1.0 else 'N/A'\n",
    "        })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cold/warm data not available for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "if cold_run and warm_run:\n",
    "    categories = []\n",
    "    cold_times = []\n",
    "    warm_times = []\n",
    "    \n",
    "    for display_name, (section, op, metric) in OPERATIONS.items():\n",
    "        cold_time = cold_run.get(section, {}).get(op, {}).get(metric, 0)\n",
    "        warm_time = warm_run.get(section, {}).get(op, {}).get(metric, 0)\n",
    "        \n",
    "        if cold_time > 0 or warm_time > 0:\n",
    "            categories.append(display_name)\n",
    "            cold_times.append(cold_time)\n",
    "            warm_times.append(warm_time)\n",
    "    \n",
    "    if categories:\n",
    "        fig = go.Figure(data=[\n",
    "            go.Bar(name='Cold Run', x=categories, y=cold_times, marker_color='lightblue'),\n",
    "            go.Bar(name='Warm Run', x=categories, y=warm_times, marker_color='lightgreen')\n",
    "        ])\n",
    "        \n",
    "        fig.update_layout(\n",
    "            title='Cold vs Warm Run Latency Comparison',\n",
    "            xaxis_title='Operation',\n",
    "            yaxis_title='Time (ms)',\n",
    "            barmode='group',\n",
    "            height=400\n",
    "        )\n",
    "        fig.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cold/warm data not available for chart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize speedup with default value - allows safe checking in later cells\n",
    "speedup = None\n",
    "\n",
    "if cold_run and warm_run:\n",
    "    cold_time = cold_run.get('ctx', {}).get('build', {}).get('total_time_ms', 0)\n",
    "    warm_time = warm_run.get('ctx', {}).get('build', {}).get('total_time_ms', 0)\n",
    "    \n",
    "    if warm_time > 0:\n",
    "        speedup = cold_time / warm_time\n",
    "        \n",
    "        # Gauge range: 0.5 (slowdown) to max(3.0, speedup*1.2) (excellent speedup)\n",
    "        # Steps: <1.0 is slowdown (gray), >1.0 is improvement (green)\n",
    "        fig = go.Figure(go.Indicator(\n",
    "            mode=\"gauge+number\",\n",
    "            value=speedup,\n",
    "            title={'text': f\"Warm vs Cold Speedup (Context Build)\"},\n",
    "            gauge={\n",
    "                'axis': {'range': [0.5, max(3.0, speedup * 1.2)]},\n",
    "                'bar': {'color': \"green\"},\n",
    "                'steps': [\n",
    "                    {'range': [0.5, 1.0], 'color': \"lightgray\"},\n",
    "                    {'range': [1.0, 2.0], 'color': \"lightgreen\"},\n",
    "                ],\n",
    "            }\n",
    "        ))\n",
    "        fig.update_layout(height=400)\n",
    "        fig.show()\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcc8 Performance Improvement\")\n",
    "        print(f\"Cold Build Time:  {cold_time}ms\")\n",
    "        print(f\"Warm Build Time:  {warm_time}ms\")\n",
    "        print(f\"Speedup:          {speedup:.2f}x\")\n",
    "    else:\n",
    "        print(\"\u26a0\ufe0f  Warm time is 0, cannot calculate speedup\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  Cold/warm data not available for speedup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## 7. Token Usage Analysis\n",
    "\n",
    "Evaluates token consumption patterns and retrieval efficiency across commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_usage = events_df.groupby('cmd')[['total_tokens', 'retrieved_tokens']].sum()\n",
    "token_usage = token_usage.sort_values('total_tokens', ascending=False)\n",
    "token_usage = token_usage[token_usage['total_tokens'] > 0]\n",
    "\n",
    "# Safe division: use where() to handle edge cases\n",
    "token_usage['efficiency'] = (\n",
    "    token_usage['retrieved_tokens'] / token_usage['total_tokens'] * 100\n",
    ").where(token_usage['total_tokens'] > 0, 0).round(1)\n",
    "token_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not token_usage.empty:\n",
    "    fig = px.bar(\n",
    "        x=token_usage.index,\n",
    "        y=token_usage['total_tokens'],\n",
    "        title='Total Token Usage by Command',\n",
    "        labels={'x': 'Command', 'y': 'Total Tokens'},\n",
    "        text=token_usage['total_tokens']\n",
    "    )\n",
    "    fig.update_traces(textposition='outside')\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No token data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_token_stats = events_df[events_df['total_tokens'] > 0].groupby('cmd').agg({\n",
    "    'total_tokens': 'sum',\n",
    "    'retrieved_tokens': 'sum',\n",
    "    'run_id': 'count'\n",
    "}).rename(columns={'run_id': 'count'})\n",
    "\n",
    "cmd_token_stats = cmd_token_stats.reset_index()\n",
    "\n",
    "if not cmd_token_stats.empty:\n",
    "    fig = px.scatter(\n",
    "        cmd_token_stats,\n",
    "        x='total_tokens',\n",
    "        y='retrieved_tokens',\n",
    "        size='count',\n",
    "        hover_data=['cmd'],\n",
    "        title='Token Efficiency: Total vs Retrieved',\n",
    "        labels={'total_tokens': 'Total Tokens', 'retrieved_tokens': 'Retrieved Tokens', 'count': 'Event Count'}\n",
    "    )\n",
    "    max_tokens = cmd_token_stats['total_tokens'].max()\n",
    "    fig.add_shape(\n",
    "        type=\"line\", \n",
    "        x0=0, y0=0, \n",
    "        x1=max_tokens, y1=max_tokens,\n",
    "        line=dict(dash=\"dash\", color=\"red\")\n",
    "    )\n",
    "    fig.update_layout(height=500)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f  No token data available for chart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 8. Timeline Analysis\n",
    "\n",
    "Examines event patterns and command usage over time to identify trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_counts = events_df.groupby('date').size().reset_index()\n",
    "daily_counts.columns = ['date', 'count']\n",
    "\n",
    "fig = px.line(\n",
    "    daily_counts,\n",
    "    x='date',\n",
    "    y='count',\n",
    "    title='Events Over Time',\n",
    "    labels={'date': 'Date', 'count': 'Event Count'},\n",
    "    markers=True\n",
    ")\n",
    "fig.update_layout(height=400)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}