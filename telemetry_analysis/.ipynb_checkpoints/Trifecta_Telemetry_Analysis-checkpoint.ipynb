{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ”¬ Trifecta Telemetry Analysis\n",
        "\n",
        "This notebook provides comprehensive analysis of Trifecta CLI telemetry data including:\n",
        "- Performance metrics (P50/P95/P99 latencies)\n",
        "- Command usage patterns\n",
        "- Search effectiveness\n",
        "- Cold vs Warm run comparisons\n",
        "- Token usage analysis\n",
        "\n",
        "---\n",
        "## 1. Setup & Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from pathlib import Path\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration\n",
        "# Adjust SEGMENT_PATH to point to the trifecta_dope directory\n",
        "SEGMENT_PATH = Path(\"..\")  # Relative to telemetry_analysis/\n",
        "DAYS = 7  # Set to 0 for all data, or N for last N days\n",
        "\n",
        "print(f\"Segment path: {SEGMENT_PATH.resolve()}\")\n",
        "print(f\"Analyzing last {DAYS} days\" if DAYS > 0 else \"Analyzing all data\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load Telemetry Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_telemetry(segment_path: Path, days: int = 7) -> pd.DataFrame:\n",
        "    \"\"\"Load telemetry events from events.jsonl into a pandas DataFrame.\"\"\"\n",
        "    tel_dir = segment_path / \"_ctx\" / \"telemetry\"\n",
        "    events_path = tel_dir / \"events.jsonl\"\n",
        "    \n",
        "    if not events_path.exists():\n",
        "        raise FileNotFoundError(f\"Telemetry file not found: {events_path}\")\n",
        "    \n",
        "    # Load events\n",
        "    events = []\n",
        "    with open(events_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    events.append(json.loads(line))\n",
        "                except json.JSONDecodeError:\n",
        "                    continue\n",
        "    \n",
        "    # Convert to DataFrame\n",
        "    df = pd.DataFrame(events)\n",
        "    \n",
        "    # Parse timestamps\n",
        "    df['timestamp'] = pd.to_datetime(df['ts'], errors='coerce')\n",
        "    df['date'] = df['timestamp'].dt.date\n",
        "    \n",
        "    # Extract nested fields\n",
        "    df['timing_ms'] = df.get('timing_ms', 0)\n",
        "    df['status'] = df['result'].apply(lambda x: x.get('status', 'unknown') if isinstance(x, dict) else 'unknown')\n",
        "    df['hits'] = df['result'].apply(lambda x: x.get('hits', 0) if isinstance(x, dict) else 0)\n",
        "    df['total_tokens'] = df['tokens'].apply(lambda x: x.get('total_tokens', 0) if isinstance(x, dict) else 0)\n",
        "    df['retrieved_tokens'] = df['tokens'].apply(lambda x: x.get('retrieved_tokens', 0) if isinstance(x, dict) else 0)\n",
        "    df['warnings_count'] = df['warnings'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
        "    \n",
        "    # Filter by date\n",
        "    if days > 0:\n",
        "        cutoff = datetime.now() - timedelta(days=days)\n",
        "        df = df[df['timestamp'] >= cutoff]\n",
        "        print(f\"Filtered to last {days} days (since {cutoff.date()})\")\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "events_df = load_telemetry(SEGMENT_PATH, DAYS)\n",
        "print(f\"\\nâœ… Loaded {len(events_df)} events\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary statistics\n",
        "print(\"ðŸ“Š Data Overview\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total Events:       {len(events_df):,}\")\n",
        "print(f\"Date Range:         {events_df['date'].min()} to {events_df['date'].max()}\")\n",
        "print(f\"Unique Commands:    {events_df['cmd'].nunique()}\")\n",
        "print(f\"Total Runs:         {events_df['run_id'].nunique()}\")\n",
        "print(f\"\\nCommand Types:\")\n",
        "print(events_df['cmd'].value_counts().to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Performance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter events with timing data\n",
        "timed = events_df[events_df['timing_ms'] > 0].copy()\n",
        "print(f\"Events with timing data: {len(timed)}\")\n",
        "\n",
        "# Calculate percentiles by command\n",
        "percentiles = timed.groupby('cmd')['timing_ms'].agg([\n",
        "    ('Count', 'count'),\n",
        "    ('P50', 'median'),\n",
        "    ('P95', lambda x: x.quantile(0.95)),\n",
        "    ('P99', lambda x: x.quantile(0.99)),\n",
        "    ('Mean', 'mean'),\n",
        "    ('Max', 'max')\n",
        "]).round(1)\n",
        "\n",
        "percentiles = percentiles.sort_values('Count', ascending=False)\n",
        "percentiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latency box plot (log scale) - Top 10 commands\n",
        "top_cmds = timed['cmd'].value_counts().head(10).index\n",
        "filtered = timed[timed['cmd'].isin(top_cmds)]\n",
        "\n",
        "fig = px.box(\n",
        "    filtered, \n",
        "    x='cmd', \n",
        "    y='timing_ms', \n",
        "    title='Latency Distribution by Command (Top 10)',\n",
        "    labels={'cmd': 'Command', 'timing_ms': 'Latency (ms)'}\n",
        ")\n",
        "fig.update_layout(yaxis_type=\"log\", height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Latency histogram\n",
        "fig = px.histogram(\n",
        "    timed,\n",
        "    x='timing_ms',\n",
        "    title='Overall Latency Distribution',\n",
        "    labels={'timing_ms': 'Latency (ms)'},\n",
        "    nbins=50,\n",
        "    log_y=True\n",
        ")\n",
        "fig.update_xaxis(type=\"log\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Command Usage Patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Command frequency pie chart\n",
        "cmd_counts = events_df['cmd'].value_counts()\n",
        "\n",
        "fig = px.pie(\n",
        "    values=cmd_counts.values, \n",
        "    names=cmd_counts.index, \n",
        "    title='Command Distribution',\n",
        "    hole=0.3\n",
        ")\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Command frequency bar chart\n",
        "fig = px.bar(\n",
        "    x=cmd_counts.index,\n",
        "    y=cmd_counts.values,\n",
        "    title='Command Frequency',\n",
        "    labels={'x': 'Command', 'y': 'Count'},\n",
        "    text=cmd_counts.values\n",
        ")\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Search Effectiveness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze ctx.search commands\n",
        "searches = events_df[events_df['cmd'] == 'ctx.search'].copy()\n",
        "\n",
        "if len(searches) > 0:\n",
        "    total_searches = len(searches)\n",
        "    with_hits = (searches['hits'] > 0).sum()\n",
        "    hit_rate = (with_hits / total_searches * 100) if total_searches > 0 else 0\n",
        "    \n",
        "    print(f\"ðŸ” Search Analysis\")\n",
        "    print(\"=\" * 50)\n",
        "    print(f\"Total Searches:      {total_searches}\")\n",
        "    print(f\"With Hits:           {with_hits} ({hit_rate:.1f}%)\")\n",
        "    print(f\"Zero Hits:           {total_searches - with_hits} ({100-hit_rate:.1f}%)\")\n",
        "    \n",
        "    # Hit rate gauge\n",
        "    fig = go.Figure(go.Indicator(\n",
        "        mode=\"gauge+number\",\n",
        "        value=hit_rate,\n",
        "        title={'text': \"Search Hit Rate (%)\"},\n",
        "        gauge={\n",
        "            'axis': {'range': [0, 100]},\n",
        "            'bar': {'color': \"darkblue\"},\n",
        "            'steps': [\n",
        "                {'range': [0, 50], 'color': \"lightgray\"},\n",
        "                {'range': [50, 80], 'color': \"gray\"},\n",
        "            ],\n",
        "            'threshold': {\n",
        "                'line': {'color': \"red\", 'width': 4},\n",
        "                'thickness': 0.75,\n",
        "                'value': 80\n",
        "            }\n",
        "        }\n",
        "    ))\n",
        "    fig.update_layout(height=400)\n",
        "    fig.show()\n",
        "else:\n",
        "    print(\"No search events found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daily search hit rate trend\n",
        "if len(searches) > 0:\n",
        "    daily_hit_rate = searches.groupby('date').apply(\n",
        "        lambda x: (x['hits'] > 0).sum() / len(x) * 100\n",
        "    ).reset_index()\n",
        "    daily_hit_rate.columns = ['date', 'hit_rate']\n",
        "    \n",
        "    fig = px.line(\n",
        "        daily_hit_rate,\n",
        "        x='date',\n",
        "        y='hit_rate',\n",
        "        title='Daily Search Hit Rate',\n",
        "        labels={'date': 'Date', 'hit_rate': 'Hit Rate (%)'},\n",
        "        markers=True\n",
        "    )\n",
        "    fig.update_layout(height=400)\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Cold vs Warm Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cold and warm run data\n",
        "tel_dir = SEGMENT_PATH / \"_ctx\" / \"telemetry\"\n",
        "\n",
        "cold_run = None\n",
        "warm_run = None\n",
        "\n",
        "cold_path = tel_dir / \"last_run_cold.json\"\n",
        "warm_path = tel_dir / \"last_run_warm.json\"\n",
        "\n",
        "if cold_path.exists():\n",
        "    with open(cold_path) as f:\n",
        "        cold_run = json.load(f)\n",
        "    print(f\"âœ… Loaded cold run data: {cold_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸  Cold run data not found\")\n",
        "\n",
        "if warm_path.exists():\n",
        "    with open(warm_path) as f:\n",
        "        warm_run = json.load(f)\n",
        "    print(f\"âœ… Loaded warm run data: {warm_path}\")\n",
        "else:\n",
        "    print(\"âš ï¸  Warm run data not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare cold vs warm runs\n",
        "if cold_run and warm_run:\n",
        "    # Extract timing data for different operations\n",
        "    operations = {\n",
        "        'Context Build': ('ctx', 'build', 'total_time_ms'),\n",
        "        'AST Parse': ('ast', 'parse', 'total_time_ms'),\n",
        "        'Context Sync': ('ctx', 'sync', 'total_time_ms'),\n",
        "    }\n",
        "    \n",
        "    comparison_data = []\n",
        "    \n",
        "    for display_name, (section, op, metric) in operations.items():\n",
        "        cold_time = cold_run.get(section, {}).get(op, {}).get(metric, 0)\n",
        "        warm_time = warm_run.get(section, {}).get(op, {}).get(metric, 0)\n",
        "        speedup = cold_time / warm_time if warm_time > 0 else 1.0\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Operation': display_name,\n",
        "            'Cold (ms)': cold_time,\n",
        "            'Warm (ms)': warm_time,\n",
        "            'Speedup': f\"{speedup:.2f}x\" if speedup != 1.0 else 'N/A'\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    comparison_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cold vs Warm comparison chart\n",
        "if cold_run and warm_run:\n",
        "    categories = []\n",
        "    cold_times = []\n",
        "    warm_times = []\n",
        "    \n",
        "    for display_name, (section, op, metric) in operations.items():\n",
        "        cold_time = cold_run.get(section, {}).get(op, {}).get(metric, 0)\n",
        "        warm_time = warm_run.get(section, {}).get(op, {}).get(metric, 0)\n",
        "        \n",
        "        if cold_time > 0 or warm_time > 0:\n",
        "            categories.append(display_name)\n",
        "            cold_times.append(cold_time)\n",
        "            warm_times.append(warm_time)\n",
        "    \n",
        "    fig = go.Figure(data=[\n",
        "        go.Bar(name='Cold Run', x=categories, y=cold_times, marker_color='lightblue'),\n",
        "        go.Bar(name='Warm Run', x=categories, y=warm_times, marker_color='lightgreen')\n",
        "    ])\n",
        "    \n",
        "    fig.update_layout(\n",
        "        title='Cold vs Warm Run Latency Comparison',\n",
        "        xaxis_title='Operation',\n",
        "        yaxis_title='Time (ms)',\n",
        "        barmode='group',\n",
        "        height=400\n",
        "    )\n",
        "    fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overall speedup gauge\n",
        "if cold_run and warm_run:\n",
        "    cold_time = cold_run.get('ctx', {}).get('build', {}).get('total_time_ms', 0)\n",
        "    warm_time = warm_run.get('ctx', {}).get('build', {}).get('total_time_ms', 0)\n",
        "    \n",
        "    if warm_time > 0:\n",
        "        speedup = cold_time / warm_time\n",
        "        \n",
        "        fig = go.Figure(go.Indicator(\n",
        "            mode=\"gauge+number\",\n",
        "            value=speedup,\n",
        "            title={'text': f\"Warm vs Cold Speedup (Context Build)\"},\n",
        "            gauge={\n",
        "                'axis': {'range': [0.5, max(3.0, speedup * 1.2)]},\n",
        "                'bar': {'color': \"green\"},\n",
        "                'steps': [\n",
        "                    {'range': [0.5, 1.0], 'color': \"lightgray\"},\n",
        "                    {'range': [1.0, 2.0], 'color': \"lightgreen\"},\n",
        "                ],\n",
        "            }\n",
        "        ))\n",
        "        fig.update_layout(height=400)\n",
        "        fig.show()\n",
        "        \n",
        "        print(f\"\\nðŸ“ˆ Performance Improvement\")\n",
        "        print(f\"Cold Build Time:  {cold_time}ms\")\n",
        "        print(f\"Warm Build Time:  {warm_time}ms\")\n",
        "        print(f\"Speedup:          {speedup:.2f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Token Usage Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token usage by command\n",
        "token_usage = events_df.groupby('cmd')[['total_tokens', 'retrieved_tokens']].sum()\n",
        "token_usage = token_usage.sort_values('total_tokens', ascending=False)\n",
        "token_usage['efficiency'] = (token_usage['retrieved_tokens'] / token_usage['total_tokens'] * 100).round(1)\n",
        "token_usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token usage bar chart\n",
        "fig = px.bar(\n",
        "    x=token_usage.index,\n",
        "    y=token_usage['total_tokens'],\n",
        "    title='Total Token Usage by Command',\n",
        "    labels={'x': 'Command', 'y': 'Total Tokens'},\n",
        "    text=token_usage['total_tokens']\n",
        ")\n",
        "fig.update_traces(textposition='outside')\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Token efficiency scatter plot\n",
        "cmd_token_stats = events_df[events_df['total_tokens'] > 0].groupby('cmd').agg({\n",
        "    'total_tokens': 'sum',\n",
        "    'retrieved_tokens': 'sum',\n",
        "    'run_id': 'count'\n",
        "}).rename(columns={'run_id': 'count'})\n",
        "\n",
        "fig = px.scatter(\n",
        "    cmd_token_stats,\n",
        "    x='total_tokens',\n",
        "    y='retrieved_tokens',\n",
        "    size='count',\n",
        "    hover_data=[cmd_token_stats.index],\n",
        "    title='Token Efficiency: Total vs Retrieved',\n",
        "    labels={'total_tokens': 'Total Tokens', 'retrieved_tokens': 'Retrieved Tokens', 'count': 'Event Count'}\n",
        ")\n",
        "fig.add_shape(type=\"line\", x0=0, y0=0, x1=cmd_token_stats['total_tokens'].max(), y1=cmd_token_stats['total_tokens'].max(),\n",
        "              line=dict(dash=\"dash\", color=\"red\"))\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Timeline Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Events over time\n",
        "daily_counts = events_df.groupby('date').size().reset_index()\n",
        "daily_counts.columns = ['date', 'count']\n",
        "\n",
        "fig = px.line(\n",
        "    daily_counts,\n",
        "    x='date',\n",
        "    y='count',\n",
        "    title='Events Over Time',\n",
        "    labels={'date': 'Date', 'count': 'Event Count'},\n",
        "    markers=True\n",
        ")\n",
        "fig.update_layout(height=400)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Command usage over time (stacked area)\n",
        "daily_cmd_counts = events_df.groupby(['date', 'cmd']).size().reset_index()\n",
        "daily_cmd_counts.columns = ['date', 'cmd', 'count']\n",
        "\n",
        "fig = px.area(\n",
        "    daily_cmd_counts,\n",
        "    x='date',\n",
        "    y='count',\n",
        "    color='cmd',\n",
        "    title='Command Usage Over Time',\n",
        "    labels={'date': 'Date', 'count': 'Event Count', 'cmd': 'Command'}\n",
        ")\n",
        "fig.update_layout(height=500)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Error & Warning Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Status breakdown\n",
        "status_counts = events_df['status'].value_counts()\n",
        "print(\"Status Breakdown:\")\n",
        "print(status_counts.to_string())\n",
        "\n",
        "# Error rate by command\n",
        "error_rate = events_df.groupby('cmd').apply(\n",
        "    lambda x: (x['status'] == 'error').sum() / len(x) * 100\n",
        ").round(1)\n",
        "error_rate = error_rate[error_rate > 0].sort_values(ascending=False)\n",
        "\n",
        "if len(error_rate) > 0:\n",
        "    print(\"\\nError Rate by Command (%):\")\n",
        "    print(error_rate.to_string())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Warnings analysis\n",
        "events_with_warnings = events_df[events_df['warnings_count'] > 0]\n",
        "print(f\"Events with warnings: {len(events_with_warnings)} ({len(events_with_warnings)/len(events_df)*100:.1f}%)\")\n",
        "\n",
        "warnings_by_cmd = events_with_warnings.groupby('cmd')['warnings_count'].agg(['sum', 'mean']).round(1)\n",
        "warnings_by_cmd.columns = ['Total Warnings', 'Avg Warnings/Event']\n",
        "warnings_by_cmd = warnings_by_cmd.sort_values('Total Warnings', ascending=False)\n",
        "\n",
        "if len(warnings_by_cmd) > 0:\n",
        "    warnings_by_cmd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Insights & Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate insights based on the data\n",
        "print(\"ðŸ” Key Insights\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Most frequent command\n",
        "top_cmd = events_df['cmd'].value_counts().index[0]\n",
        "top_cmd_pct = events_df['cmd'].value_counts().iloc[0] / len(events_df) * 100\n",
        "print(f\"1. Most used command: {top_cmd} ({top_cmd_pct:.1f}% of all events)\")\n",
        "\n",
        "# Slowest operation (P95)\n",
        "if len(timed) > 0:\n",
        "    slowest = percentiles['P95'].idxmax()\n",
        "    slowest_p95 = percentiles.loc[slowest, 'P95']\n",
        "    print(f\"2. Slowest operation (P95): {slowest} at {slowest_p95:.1f}ms\")\n",
        "\n",
        "# Search effectiveness\n",
        "if len(searches) > 0:\n",
        "    if hit_rate < 50:\n",
        "        print(f\"3. âš ï¸  Low search hit rate: {hit_rate:.1f}% - consider optimizing index\")\n",
        "    elif hit_rate > 80:\n",
        "        print(f\"3. âœ… Good search hit rate: {hit_rate:.1f}%\")\n",
        "    else:\n",
        "        print(f\"3. Search hit rate: {hit_rate:.1f}% - room for improvement\")\n",
        "\n",
        "# Cold vs Warm\n",
        "if cold_run and warm_run:\n",
        "    if speedup > 2.0:\n",
        "        print(f\"4. âœ… Excellent warm cache speedup: {speedup:.2f}x\")\n",
        "    elif speedup > 1.5:\n",
        "        print(f\"4. âœ“ Good warm cache speedup: {speedup:.2f}x\")\n",
        "    else:\n",
        "        print(f\"4. âš ï¸  Low warm cache speedup: {speedup:.2f}x - cache may not be effective\")\n",
        "\n",
        "# Error rate\n",
        "error_count = (events_df['status'] == 'error').sum()\n",
        "error_pct = error_count / len(events_df) * 100\n",
        "if error_pct > 5:\n",
        "    print(f\"5. âš ï¸  High error rate: {error_pct:.1f}% ({error_count} errors)\")\n",
        "elif error_pct > 0:\n",
        "    print(f\"5. Error rate: {error_pct:.1f}% ({error_count} errors)\")\n",
        "else:\n",
        "    print(f\"5. âœ… No errors detected\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Analysis complete! Use the interactive charts above to explore further.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}