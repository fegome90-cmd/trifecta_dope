No es por performance (sha1 es r√°pido), sino por estabilidad sem√°ntica: un cambio m√≠nimo cambia todo, obvio, pero eso est√° bien; el problema es que a veces un chunk gigante cambia por una coma y pierdes continuidad total.

‚úÖ Recomendaci√≥n pragm√°tica:
	‚Ä¢	id_seed = doc + "\n" + title_path + "\n" + sha256(text_normalized)
	‚Ä¢	id = sha1(id_seed)[:10]

As√≠ no dependes de concatenar texto crudo.

3) source_files debe incluir path + sha256 + mtime + size

Con eso puedes:
	‚Ä¢	cachear
	‚Ä¢	detectar cambios
	‚Ä¢	reproducir

4) digest NO debe ser ‚Äúprimeros chars‚Äù

En la foto ya dice ‚Äúresumen estructurado‚Äù / ‚Äúprimeras 2 secciones relevantes‚Äù. Bien.
Solo aseg√∫rate de que el digest sea peque√±o (p. ej. 10‚Äì30 l√≠neas por doc) o vuelves a quemar tokens.

5) Falta un campo clave: chunking

Agrega metadatos del m√©todo, para que el runtime sepa c√≥mo se gener√≥:

"chunking": { "method": "headings+paragraph_fallback", "max_chars": 6000 }


‚∏ª

Qu√© har√≠a yo ahora (orden exacto, sin sobre-ingenier√≠a) üß∞

Paso 1 ‚Äî Implementa el builder (solo pack)
	‚Ä¢	Entrada: 3 .md
	‚Ä¢	Salida: context_pack.json
	‚Ä¢	No metas tools aqu√≠.

Paso 2 ‚Äî Implementa runtime tool
	‚Ä¢	context.get(chunk_id) ‚Üí devuelve chunks[].text
	‚Ä¢	(opcional) context.search(query,k) ‚Üí devuelve IDs usando BM25 simple (o hasta difflib al principio)
